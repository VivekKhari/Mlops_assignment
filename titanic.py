# -*- coding: utf-8 -*-
"""Titanic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s_fwrVHQSzyus14EdNthO0S2cyljUjrO
"""

# Importing necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns

# Load your data here (assuming 'data' is already available in your notebook or you have saved somwhere else)
data = sns.load_dataset('titanic')

# Check the column names in the dataset
print("Columns in the dataset:", data.columns)

# Drop irrelevant columns based on the dataset and correlation analysis
# Write code for dropping Columns such as 'survived', 'who', 'alive', and 'alone' are considered irrelevant for predicting 'fare'


# Check the updated columns after dropping irrelevant ones
columns_to_drop = ['survived', 'who', 'alive', 'alone']
data_cleaned = data.drop(columns=columns_to_drop, errors='ignore')

print("Features in the dataset:", data_cleaned.columns)

# Define numerical and categorical features
# Numerical features include 'age', 'sibsp', 'parch', and 'family_size'
# Categorical features include 'sex', 'embarked', 'class', 'adult_male', 'deck', 'embark_town', and 'pclass'

# write your code here
numerical_features = ['age', 'sibsp', 'parch', 'fare']

# Define categorical features
categorical_features = [
    'sex', 'embarked', 'class', 'adult_male',
    'deck', 'embark_town', 'pclass'
]

# Separate features and target variable
# 'X' contains all the features (both numerical and categorical) except 'fare'
# 'y' is the target variable which is 'fare'

# write your code here
X = data[numerical_features + categorical_features].drop(columns='fare')
y = data['fare']

X.columns

numerical_features

# Handle categorical variables by one-hot encoding
# This will convert categorical features into numerical format using one-hot encoding
X = pd.get_dummies(X, columns=categorical_features)


# Standardize the numerical features
# StandardScaler will normalize numerical features to have a mean of 0 an d a standard deviation of 1
scaler = StandardScaler()
numerical_features = ['age', 'sibsp', 'parch']
X[numerical_features] = scaler.fit_transform(X[numerical_features])

X['age'].fillna(X['age'].mean(), inplace=True)

X.isnull().sum()

# Split the dataset into training and testing sets
# 80% of the data will be used for training, and 20% for testing
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)

# Initialize the Linear Regression model
model = LinearRegression()

# Fit the model on the training data
model.fit(X_train, y_train)

# Predict the fare on the test data
y_pred = model.predict(X_test)

# Evaluate the model performance
# Calculate Mean Squared Error, R-squared, and Mean Absolute Error
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")
print(f"Mean Absolute Error: {mae}")

# Visualizing actual vs predicted fares
plt.figure(figsize=(8,6))
plt.scatter(y_test, y_pred, color="blue", label="Predicted")
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linewidth=2, label="Ideal Fit")
plt.xlabel('Actual Fare')
plt.ylabel('Predicted Fare')
plt.savefig('ActualvsPredicted.png')
plt.title('Actual vs Predicted Fare')
plt.legend()
plt.show()
